<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-03-06T13:22:56-08:00</updated><id>http://localhost:4000//</id><title type="html">Hello big, big, BIG World</title><subtitle>Wow, this thing actually works. Pretty cool isn't it. I live in _config.yml.  Whoever google searches, they will find me here.
</subtitle><entry><title type="html">Some interesting properties of vectorization</title><link href="http://localhost:4000/tensor/2017/03/06/vectorize-prop/" rel="alternate" type="text/html" title="Some interesting properties of vectorization" /><published>2017-03-06T11:34:08-08:00</published><updated>2017-03-06T11:34:08-08:00</updated><id>http://localhost:4000/tensor/2017/03/06/vectorize-prop</id><content type="html" xml:base="http://localhost:4000/tensor/2017/03/06/vectorize-prop/">&lt;div class=&quot;post-title&quot;&gt;
  
  
  
  &lt;h1&gt;Some interesting properties of vectorization&lt;/h1&gt;
  
  &lt;div class=&quot;post-info&quot;&gt;Created by
    &lt;a href=&quot;mailto:horacetso@gmail.com&quot;&gt;Horace W Tso&lt;/a&gt;
    on March 06, 2017.
    
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In the previous post, I introduce a procedure that turns a matrix into a vector. It’s just stacking the column to form a longer vector. Naturally, you’d ask what properties of the matrix are preserved in its “linear” incarnation.&lt;/p&gt;

&lt;p&gt;The first property that carries over to its vector form is its norm. It’s totally intuitive but still worth stating.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Proposition 1 : For any matrix &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;,&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\|M\|_{F} = \|v\|_2&lt;/script&gt;

  &lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;v = vec(M)&lt;/script&gt;, the subscript &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; stands for the Frobenius norm.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A direct corollary of this is the following statement, which is a little bit less intuitive, but still just a consequence of the Prop 1.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Proposition 2 : If matrix M has rank 1, then&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\|v\|_2 = \sigma_1&lt;/script&gt;

  &lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\sigma_1&lt;/script&gt; is the largest and the only non-zero singular value of &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To understand this statement, I like to visualize it this way. In the vector space that &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; lives in, which is a higher dimension space than the column space of &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;, there is a nice regularity there. No matter how the matrix is chosen, as long as it is rank-1, the vector must lie on the ball with a radius equal to the singular value of &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;.&lt;/p&gt;</content><summary type="html">Some interesting properties of vectorization
  
  Created by
    Horace W Tso
    on March 06, 2017.</summary></entry><entry><title type="html">What’s the probability stock market just hit a permanent top</title><link href="http://localhost:4000/financial/markets/2017/03/02/stock-market-topped/" rel="alternate" type="text/html" title="What's the probability stock market just hit a permanent top" /><published>2017-03-02T12:29:00-08:00</published><updated>2017-03-02T12:29:00-08:00</updated><id>http://localhost:4000/financial/markets/2017/03/02/stock-market-topped</id><content type="html" xml:base="http://localhost:4000/financial/markets/2017/03/02/stock-market-topped/">&lt;div class=&quot;post-title&quot;&gt;
  
  
  
  &lt;h1&gt;What's the probability stock market just hit a permanent top&lt;/h1&gt;
  
  &lt;div class=&quot;post-info&quot;&gt;Created by
    &lt;a href=&quot;mailto:horacetso@gmail.com&quot;&gt;Horace W Tso&lt;/a&gt;
    on March 02, 2017.
    
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The Dow Jones Industrial Average reached a record yesterday, an intraday high of 21,165, to be exact. For the faithful and the detractors, it’s provocative to ask, what’s the chance we’ve just witnessed a permanent top. I mean, from now till end of the world, investors would never see this level again.&lt;/p&gt;

&lt;p&gt;Wise men of stock market would tell you that no one knows the absolute peak (and trough) until long after the fact. That’s what make timing such an intriguing endeavour.&lt;/p&gt;

&lt;p&gt;Okey, now you may ask, what basis supports any analysis and knowledge of a market top. I could resort to the dark art of technical analysis and give you a dozen reasons; Overbought, divergence, you name it.&lt;/p&gt;

&lt;p&gt;[…to be continued.]&lt;/p&gt;</content><summary type="html">What's the probability stock market just hit a permanent top
  
  Created by
    Horace W Tso
    on March 02, 2017.</summary></entry><entry><title type="html">How to hack the package dependency error in R</title><link href="http://localhost:4000/r_hacking/2016/12/06/hack-dep-err/" rel="alternate" type="text/html" title="How to hack the package dependency error in R" /><published>2016-12-06T11:34:08-08:00</published><updated>2016-12-06T11:34:08-08:00</updated><id>http://localhost:4000/r_hacking/2016/12/06/hack-dep-err</id><content type="html" xml:base="http://localhost:4000/r_hacking/2016/12/06/hack-dep-err/">&lt;div class=&quot;post-title&quot;&gt;
  
  
  
  &lt;h1&gt;How to hack the package dependency error in R&lt;/h1&gt;
  
  &lt;div class=&quot;post-info&quot;&gt;Created by
    &lt;a href=&quot;mailto:horacetso@gmail.com&quot;&gt;Horace W Tso&lt;/a&gt;
    on December 06, 2016.
    
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;You want to install a package, but R complains that it doesn’t like the version of R you got. The error message read something like this,&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ERROR: this R is version 3.2.1, package 'XYZ' requires R &amp;gt;=  3.2.2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Typically, all you need to do is to upgrade your current version of R. But there are times when this is not desirable, or not even possilbe. Here is a hack you can try.&lt;/p&gt;

&lt;p&gt;WARNING : There’s usually a good reason a package depends on a particular version of R. So if yours is way out of line of currency, this may not work. So be warned!&lt;/p&gt;

&lt;p&gt;First, go to CRAN and download the source of the package, which is typically a zipped file with extension &lt;code class=&quot;highlighter-rouge&quot;&gt;.tar.gz&lt;/code&gt;. For example, the source file for the package &lt;code class=&quot;highlighter-rouge&quot;&gt;ModelMetrics&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;ModelMetrics_1.1.0.tar.gz&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Uncompress it into a local directory. In most platforms (Windows, linux), it’s just a matter of double clicking it and an unzip app would open it in a window.&lt;/p&gt;

&lt;p&gt;In the top level of the folder, you should see DESCRIPTION, a text file that you can open with any text editor. Open it and you will find a line&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Depends: R (&amp;gt;= 3.2.2)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now, change the version to your current version 3.2.1. Save it. And install this local copy of the package. In R console, type&lt;/p&gt;

&lt;pre class=&quot;terminal&quot;&gt;&lt;code&gt;install.packages(&quot;/local/path/to/ThePackage&quot;, repo=NULL, type=&quot;source&quot;)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Voila! The dependency error should go away and you should be able to complete the install.&lt;/p&gt;</content><summary type="html">How to hack the package dependency error in R
  
  Created by
    Horace W Tso
    on December 06, 2016.</summary></entry><entry><title type="html">Vectorize matrices in C algorithms</title><link href="http://localhost:4000/c_coding/2016/10/19/matrix2vector/" rel="alternate" type="text/html" title="Vectorize matrices in C algorithms" /><published>2016-10-19T12:34:08-07:00</published><updated>2016-10-19T12:34:08-07:00</updated><id>http://localhost:4000/c_coding/2016/10/19/matrix2vector</id><content type="html" xml:base="http://localhost:4000/c_coding/2016/10/19/matrix2vector/">&lt;div class=&quot;post-title&quot;&gt;
  
  
  
  &lt;h1&gt;Vectorize matrices in C algorithms&lt;/h1&gt;
  
  &lt;div class=&quot;post-info&quot;&gt;Created by
    &lt;a href=&quot;mailto:horacetso@gmail.com&quot;&gt;Horace W Tso&lt;/a&gt;
    on October 19, 2016.
    
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In case you wonder who on earth would want to turn a matrix into a vector, I’d give you two reasons.&lt;/p&gt;

&lt;p&gt;The first is about coding. Writing aglorithms in C has tremendous speed advantage, but a steep price to pay in dealing with certain mathematical objects, such as matrices. Many libraries out there have the infrastructure to handle matrices, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;boost&lt;/code&gt;. But in core C, it’s not so easy. I want to talk about coding with just the native C vector.&lt;/p&gt;

&lt;p&gt;The second is about a connection with tensor mathematics. It turns out collapsing a matrix into a vector uses the same operator that reshape a higher order tensor to a matrix. In a later post, I will explain the reason for this type of transformation, and show some of its nice properties.&lt;/p&gt;

&lt;p&gt;A matrix is an object where a value is placed in a two-dimensional “coordinate”. You can think of it as a map; if you give me a point in space, I’d’ tell you the value there.&lt;/p&gt;

&lt;p&gt;Suppose I have a matrix &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; of size &lt;script type=&quot;math/tex&quot;&gt;T \times K&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is the number of rows (time steps)
and &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; the number of columns (hidden states),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{M} = \left.\left( 
   \vphantom{\begin{array}{c}1\\1\\1\\1\end{array}}
      \smash{\underbrace{
         \begin{array}{ccccc}
           x_1&amp; y_1 &amp;\cdots &amp;z_1\\
           x_2&amp; y_2 &amp;\cdots &amp;z_2\\
         \vdots&amp;&amp;\ddots&amp;\\
           x_T&amp; y_T&amp; \cdots &amp;z_T
           \end{array}
            }_{K\text{ columns}}}
         \right)\right\}
    \,T\text{ rows} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;&lt;/script&gt;

&lt;p&gt;and I want to represent this matrix as a vector. I could stack the &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; columns together, or I could stack the &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; rows to make a vector. I choose the later so that the states form a contagious block in every time step. This makes it easier to access in a loop. The vector has length &lt;script type=&quot;math/tex&quot;&gt;T*K&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v = ( \underbrace{x_1, x_2 \ldots, x_T}_{T}, \underbrace{y_1, y_2, \ldots, y_T}_{T}, \ldots, \underbrace{z_1, \ldots, z_T}_{T} )&lt;/script&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; code to do this conversion is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\texttt{v} = \texttt{as.double(t(M))}&lt;/script&gt;

&lt;p&gt;To read an element of the orignal matrix from this vector, we need to map the 
row and column coordinates to a linear offset. The mapping is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(t, i) \; \longleftrightarrow \; \texttt{j = i*T + t}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the row index, &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; the column index of the matrix &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; is the corresponding
element in the vector. For a C-style zero-based vector, the mapping is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(t, i) \; \longleftrightarrow \; \texttt{j} = (i-1)*T + (t-1)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;t = 1, \ldots, T&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;i = 1,\ldots,K&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\texttt{j} =0, 1, \ldots, T*K -1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For example as illustration, the element at row 6th and column 4th is accessed by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
  M[6,4] &amp;= \texttt{v[3*T + 5]}
  \\M[1,1] &amp;= \texttt{v[0]} 
  \\M[T,K] &amp;= \texttt{v[(K-1)*T + T]}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is the vectorizing mechanics.  See my next blog on the implication of this procedure.&lt;/p&gt;</content><summary type="html">Vectorize matrices in C algorithms
  
  Created by
    Horace W Tso
    on October 19, 2016.</summary></entry><entry><title type="html">Norm property of joint probability distribution</title><link href="http://localhost:4000/probabiliity/2012/12/25/prob-norm/" rel="alternate" type="text/html" title="Norm property of joint probability distribution" /><published>2012-12-25T00:00:00-08:00</published><updated>2012-12-25T00:00:00-08:00</updated><id>http://localhost:4000/probabiliity/2012/12/25/prob-norm</id><content type="html" xml:base="http://localhost:4000/probabiliity/2012/12/25/prob-norm/">&lt;div class=&quot;post-title&quot;&gt;
  
  
  
  &lt;h1&gt;Norm property of joint probability distribution&lt;/h1&gt;
  
  &lt;div class=&quot;post-info&quot;&gt;Created by
    &lt;a href=&quot;mailto:horacetso@gmail.com&quot;&gt;Horace W Tso&lt;/a&gt;
    on December 25, 2012.
    
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In machine learning we encounter joint probability distribution on day one of work. For example, the MNIST digit classification problem is really about estimating the joint probability of the 10 digit labels and the image pixels. So, needless to say, the joint density in general is of great interest. Without anything specific to a problem, what can we say about a joint probability function. It turns out there is an interesting property which I’m going to show you in this post.&lt;/p&gt;

&lt;p&gt;I’ll limit myself to discrete random variables here. So, we’re looking at the joint probability of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; takes value in the integer set &lt;script type=&quot;math/tex&quot;&gt;\{1,2,\ldots, n\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;\{1,2,\dots,m\}&lt;/script&gt;. The joint probability &lt;script type=&quot;math/tex&quot;&gt;p(x,y)&lt;/script&gt; can be represented as a matrix of size &lt;script type=&quot;math/tex&quot;&gt;n \times m&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
[P]_{ij} = 
         \begin{bmatrix}
           a_{11}&amp;\cdots &amp;a_{1m}\\
           a_{21}&amp;\cdots &amp;a_{2m}\\
           \vdots&amp;\ddots &amp; \\
           a_{n1}&amp;\cdots &amp;a_{nm}
           \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;[P]_{ij} = p(x=i, y=j)&lt;/script&gt;. So, each entry in the matrix is the probability that &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; take the respective value jointly (“and” is the key word).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lemma 1 : &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; has rank one if and only if &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; are independent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The proof is very simple and I won’t go into it here (see my writeup). But I want to bring out the intuition. If two random variables are independent, their joint density is just the product of the marginals, i.e. &lt;script type=&quot;math/tex&quot;&gt;p(x,y) = p(x)p(y)&lt;/script&gt;. This is precisely the property that a rank-1 matrix has. A rank-1 matrix is the cross product of two vectors (singular vectors) scaled by a constant, which is its singular value. The singular value in this case is 1.0.&lt;/p&gt;

&lt;p&gt;So, what’s the big deal?&lt;/p&gt;

&lt;p&gt;There is a theorem (in fact it’s the most important result in linear algebra) that says a matrix could be decomposed into a sum of vector cross-products. If we perform SVD on the probability matrix, the singular values tell us how close the random variables are to independence.&lt;/p&gt;

&lt;p&gt;For example, if the largest singular value is 10.0, the second largest is 1.0, we could say the joint is pretty close to independence. On the other hand, if the first two singular values are 10.0 and 9.5, then &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; have stronger interdependence.&lt;/p&gt;

&lt;p&gt;I hope this makes sense. Let me know if you have questions.&lt;/p&gt;</content><summary type="html">Norm property of joint probability distribution
  
  Created by
    Horace W Tso
    on December 25, 2012.</summary></entry></feed>
